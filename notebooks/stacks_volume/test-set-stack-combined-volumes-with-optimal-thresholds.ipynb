{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import dicom\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "from skimage import transform\n",
    "from sklearn.externals import joblib\n",
    "from scipy import ndimage\n",
    "from matplotlib import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_filepaths_dict(dataset='test'):\n",
    "    if dataset == 'train':\n",
    "        nrange = range(1, 501)\n",
    "    elif dataset == 'validate':\n",
    "        nrange = range(501, 701)\n",
    "    elif dataset == 'test':\n",
    "        nrange = range(701, 1141)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    data_path = '../../data/test'\n",
    "\n",
    "    slice_filepaths = {}\n",
    "    for i in tqdm(nrange):\n",
    "        filepaths = {}\n",
    "        folders = natsorted([(x.name, os.path.abspath(x.path)) for x in os.scandir('{}/{}/study'.format(data_path, i)) \\\n",
    "                             if x.is_dir()])\n",
    "        for folder_name, folder_path in folders:\n",
    "            files = natsorted([(x.name, os.path.abspath(x.path)) for x in os.scandir(folder_path) \\\n",
    "                               if x.is_file() and x.name.endswith('.dcm')])\n",
    "            filepaths[folder_name] = files\n",
    "        slice_filepaths[i] = filepaths\n",
    "\n",
    "    return slice_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "filepaths_test = create_filepaths_dict(dataset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_label(pt, mode='ED'):\n",
    "    if mode == 'ES':\n",
    "        return systole_labels[pt] < np.arange(600)\n",
    "    elif mode == 'ED':\n",
    "        return diastole_labels[pt] < np.arange(600)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "        \n",
    "def apply_window(arr, window_center, window_width):\n",
    "    return np.clip(arr, window_center - window_width/2, window_center + window_width/2)\n",
    "\n",
    "\n",
    "def apply_per_slice_norm(arr):\n",
    "    mean = np.mean(arr.ravel())\n",
    "    std = np.std(arr.ravel())\n",
    "    if std == 0:\n",
    "        return np.zeros(arr.shape)\n",
    "    return (arr - mean) / std\n",
    "\n",
    "\n",
    "def crop_to_square(arr, size):\n",
    "    x_len, y_len = arr.shape\n",
    "    shorter_len = min(x_len, y_len)\n",
    "    x_start = (arr.shape[0] - shorter_len) // 2\n",
    "    x_end = x_start + shorter_len\n",
    "    y_start = (arr.shape[1] - shorter_len) // 2\n",
    "    y_end = y_start + shorter_len\n",
    "    return transform.resize(arr[x_start:x_end, y_start:y_end], \n",
    "                            (size, size), order=1, clip=True, preserve_range=True)\n",
    "\n",
    "\n",
    "def crop_to_square_normalized(img_orig, pixel_spacing, size):\n",
    "    img_norm = ndimage.interpolation.zoom(img_orig, [float(x) for x in pixel_spacing], order=0, mode='constant')\n",
    "    \n",
    "    length_x, length_y = img_norm.shape\n",
    "    if length_x >= size:\n",
    "        x_start = length_x // 2 - size // 2\n",
    "        x_end = length_x // 2 + size // 2\n",
    "    else:\n",
    "        x_start = 0\n",
    "        x_end = length_x\n",
    "    if length_y >= size:\n",
    "        y_start = length_y // 2 - size // 2\n",
    "        y_end = length_y // 2 + size // 2\n",
    "    else:\n",
    "        y_start = 0\n",
    "        y_end = length_y\n",
    "    \n",
    "    img_new = np.zeros((size, size))\n",
    "    new_x_shift = (size - (x_end - x_start)) // 2\n",
    "    new_y_shift = (size - (y_end - y_start)) // 2\n",
    "    img_new[new_x_shift:(new_x_shift + x_end - x_start), \n",
    "            new_y_shift:(new_y_shift + y_end - y_start)] = img_norm[x_start:x_end, y_start:y_end]\n",
    "    \n",
    "    return img_new\n",
    "\n",
    "\n",
    "def localize_to_centroid(img, centroid, width_about_centroid):\n",
    "    # assumes already cropped to square\n",
    "    x, y = centroid\n",
    "    x = int(round(x))\n",
    "    y = int(round(y))\n",
    "    x_start = x - width_about_centroid // 2\n",
    "    x_end = x + width_about_centroid // 2\n",
    "    y_start = y - width_about_centroid // 2\n",
    "    y_end = y + width_about_centroid // 2\n",
    "    \n",
    "    if x_start < 0:\n",
    "        x_end += (0 - x_start)\n",
    "        x_start = 0\n",
    "    if x_end > img.shape[0]:\n",
    "        x_start -= (img.shape[0] - x_end)\n",
    "        x_end = img.shape[0]\n",
    "    if y_start < 0:\n",
    "        y_end += (0 - y_start)\n",
    "        y_start = 0\n",
    "    if y_end > img.shape[1]:\n",
    "        y_start -= (img.shape[1] - y_end)\n",
    "        y_end = img.shape[1]\n",
    "        \n",
    "    return img[x_start:x_end, y_start:y_end], (x_start, x_end), (y_start, y_end)\n",
    "\n",
    "\n",
    "def normalized_z_loc(df):\n",
    "    # assumes patient position HFS\n",
    "    \n",
    "    position = [float(s) for s in df.ImagePositionPatient]\n",
    "    orientation = [float(s) for s in df.ImageOrientationPatient]\n",
    "    \n",
    "    # first voxel coordinates from DICOM ImagePositionPatient field\n",
    "    x_loc, y_loc, z_loc = position\n",
    "    \n",
    "    # row/column direction cosines from DICOM ImageOrientationPatient field\n",
    "    row_dircos_x, row_dircos_y, row_dircos_z, col_dircos_x, col_dircos_y, col_dircos_z = orientation\n",
    "    \n",
    "    # normalized direction cosines\n",
    "    dircos_x = row_dircos_y * col_dircos_z - row_dircos_z * col_dircos_y\n",
    "    dircos_y = row_dircos_z * col_dircos_x - row_dircos_x * col_dircos_z\n",
    "    dircos_z = row_dircos_x * col_dircos_y - row_dircos_y * col_dircos_x\n",
    "    \n",
    "    # z-coordinate location recalculated based on reference\n",
    "    z_loc_norm = dircos_x * x_loc + dircos_y * y_loc + dircos_z * z_loc\n",
    "    return z_loc_norm\n",
    "\n",
    "\n",
    "def get_all_series_filepaths(filepaths):\n",
    "    t_slices = 30\n",
    "    \n",
    "    # create sax series filepaths\n",
    "    # handles irregularies such as those including z-slices and t-slices in the same folder\n",
    "    series_filepaths_all = []\n",
    "    for view in filepaths.keys(): \n",
    "        if not re.match(r'^sax', view):\n",
    "            continue\n",
    "        \n",
    "        if len(filepaths[view]) == t_slices:\n",
    "            series_filepaths_all.append(filepaths[view])\n",
    "        elif len(filepaths[view]) < t_slices:\n",
    "            series_filepaths_all.append(filepaths[view][:] + filepaths[view][:(t_slices - len(filepaths[view]))])\n",
    "        else:\n",
    "            if re.match(r'^\\w+-\\d+-\\d+-\\d+.dcm$', filepaths[view][0][0]) is not None:\n",
    "                series_filepaths_split = []\n",
    "                slices_list = []\n",
    "                series_filepaths_sort_by_slice = sorted(filepaths[view][:], \n",
    "                                                        key=lambda x: '{}-{}'.format(x[0].split('-')[-1].split('.')[0], \n",
    "                                                                                     x[0].split('-')[-2]))\n",
    "                for fname, fpath in series_filepaths_sort_by_slice:\n",
    "                    nslice = fname.split('-')[-1].split('.')[0]\n",
    "                    tframe = fname.split('-')[-2]\n",
    "                    if nslice not in slices_list:\n",
    "                        if len(series_filepaths_split) == t_slices:\n",
    "                            series_filepaths_all.append(series_filepaths_split)\n",
    "                        elif len(series_filepaths_split) < t_slices and len(series_filepaths_split) > 0:\n",
    "                            series_filepaths_all.append((series_filepaths_split[:] + \n",
    "                                                         series_filepaths_split[:(t_slices - len(series_filepaths_split))]))\n",
    "                        series_filepaths_split = []\n",
    "                        series_filepaths_split.append((fname, fpath))\n",
    "                        slices_list.append(nslice)\n",
    "                    else:\n",
    "                        series_filepaths_split.append((fname, fpath))\n",
    "                        \n",
    "    return series_filepaths_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ES frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mean_percent_diff_seq(series_filepaths, img_size=256):\n",
    "    nb_frames = 30\n",
    "    if len(series_filepaths) != nb_frames:\n",
    "        return None\n",
    "\n",
    "    mean_diff_seq = []\n",
    "\n",
    "    fname, fpath = series_filepaths[0]\n",
    "    df = dicom.read_file(fpath)\n",
    "    img_ED = apply_per_slice_norm(crop_to_square_normalized(df.pixel_array, df.PixelSpacing, img_size))\n",
    "    for fname, fpath in series_filepaths[1:]:\n",
    "        differences = []\n",
    "\n",
    "        df = dicom.read_file(fpath)\n",
    "        img_frame = apply_per_slice_norm(crop_to_square_normalized(df.pixel_array, df.PixelSpacing, img_size))\n",
    "        img_percent_diff = (img_frame - img_ED) / (img_ED + 1e-6)\n",
    "        differences.append(np.mean(img_percent_diff))\n",
    "        differences.append(np.mean(np.abs(img_percent_diff)))\n",
    "\n",
    "        for scaling in [2, 4, 8]:\n",
    "            for i in range(scaling):\n",
    "                for j in range(scaling):\n",
    "                    differences.append(np.mean(img_percent_diff[(i*img_size//scaling):((i+1)*img_size//scaling),\n",
    "                                                                (i*img_size//scaling):((i+1)*img_size//scaling)],\n",
    "                                               axis=None))\n",
    "                    differences.append(np.mean(np.abs(img_percent_diff[(i*img_size//scaling):((i+1)*img_size//scaling),\n",
    "                                                                       (i*img_size//scaling):((i+1)*img_size//scaling)]),\n",
    "                                               axis=None))\n",
    "\n",
    "        mean_diff_seq.append(differences)\n",
    "\n",
    "    return np.array(mean_diff_seq).astype(np.float32)\n",
    "\n",
    "def predict_es_frame(filepaths):\n",
    "    clf = joblib.load('../../model_weights/ES_detection_GBR.pkl')\n",
    "    series_filepaths_all = get_all_series_filepaths(filepaths)\n",
    "    data = []\n",
    "    for series_filepaths in series_filepaths_all:\n",
    "        data.append(create_mean_percent_diff_seq(series_filepaths, img_size=256))\n",
    "    preds = clf.predict([x.ravel() for x in data])\n",
    "    return np.round(np.mean(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 3: Tesla K80 (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Graph\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten, Merge, Reshape, Lambda\n",
    "from keras.layers.core import TimeDistributedDense, TimeDistributedMerge\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ParametricSoftplus, ELU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.noise import GaussianDropout, GaussianNoise\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import initializations\n",
    "from keras.layers.core import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "# for preventing python max recursion limit error\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=None, keepdims=False))\n",
    "\n",
    "def binaryCE(y_true, y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_pred, y_true), axis=None, keepdims=False)\n",
    "\n",
    "class Rotate90(Layer):\n",
    "    def __init__(self, direction='clockwise', **kwargs):\n",
    "        super(Rotate90, self).__init__(**kwargs)\n",
    "        self.direction = direction\n",
    "\n",
    "    def get_output(self, train):\n",
    "        X = self.get_input(train)\n",
    "        if self.direction == 'clockwise':\n",
    "            return X.transpose((0, 2, 1))[:, :, ::-1]\n",
    "        elif self.direction == 'counterclockwise':\n",
    "            return X.transpose((0, 2, 1))[:, ::-1, :]\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"name\": self.__class__.__name__}\n",
    "        base_config = super(Rotate90, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LV localization net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LV_loc_model = Graph()\n",
    "\n",
    "LV_loc_model.add_input(name='input', input_shape=(1, 256, 256))\n",
    "\n",
    "LV_loc_model.add_node(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-1-1', input='input')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-1-1-bn', input='conv-1-1')\n",
    "LV_loc_model.add_node(ELU(), name='conv-1-1-activ', input='conv-1-1-bn')\n",
    "LV_loc_model.add_node(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-1-2', input='conv-1-1-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-1-2-bn', input='conv-1-2')\n",
    "LV_loc_model.add_node(ELU(), name='conv-1-2-activ', input='conv-1-2-bn')\n",
    "LV_loc_model.add_node(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'),\n",
    "               name='pool-1', input='conv-1-2-activ')\n",
    "\n",
    "LV_loc_model.add_node(Convolution2D(64, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-2-1', input='pool-1')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-2-1-bn', input='conv-2-1')\n",
    "LV_loc_model.add_node(ELU(), name='conv-2-1-activ', input='conv-2-1-bn')\n",
    "LV_loc_model.add_node(Convolution2D(64, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-2-2', input='conv-2-1-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-2-2-bn', input='conv-2-2')\n",
    "LV_loc_model.add_node(ELU(), name='conv-2-2-activ', input='conv-2-2-bn')\n",
    "LV_loc_model.add_node(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'),\n",
    "               name='pool-2', input='conv-2-2-activ')\n",
    "\n",
    "LV_loc_model.add_node(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-3-1', input='pool-2')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-3-1-bn', input='conv-3-1')\n",
    "LV_loc_model.add_node(ELU(), name='conv-3-1-activ', input='conv-3-1-bn')\n",
    "LV_loc_model.add_node(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-3-2', input='conv-3-1-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-3-2-bn', input='conv-3-2')\n",
    "LV_loc_model.add_node(ELU(), name='conv-3-2-activ', input='conv-3-2-bn')\n",
    "LV_loc_model.add_node(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-3-3', input='conv-3-2-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-3-3-bn', input='conv-3-3')\n",
    "LV_loc_model.add_node(ELU(), name='conv-3-3-activ', input='conv-3-3-bn')\n",
    "LV_loc_model.add_node(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'),\n",
    "               name='pool-3', input='conv-3-3-activ')\n",
    "\n",
    "LV_loc_model.add_node(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-4-1', input='pool-3')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-4-1-bn', input='conv-4-1')\n",
    "LV_loc_model.add_node(ELU(), name='conv-4-1-activ', input='conv-4-1-bn')\n",
    "LV_loc_model.add_node(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-4-2', input='conv-4-1-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-4-2-bn', input='conv-4-2')\n",
    "LV_loc_model.add_node(ELU(), name='conv-4-2-activ', input='conv-4-2-bn')\n",
    "LV_loc_model.add_node(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-4-3', input='conv-4-2-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-4-3-bn', input='conv-4-3')\n",
    "LV_loc_model.add_node(ELU(), name='conv-4-3-activ', input='conv-4-3-bn')\n",
    "LV_loc_model.add_node(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'),\n",
    "               name='pool-4', input='conv-4-3-activ')\n",
    "\n",
    "LV_loc_model.add_node(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-5-1', input='pool-4')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-5-1-bn', input='conv-5-1')\n",
    "LV_loc_model.add_node(ELU(), name='conv-5-1-activ', input='conv-5-1-bn')\n",
    "LV_loc_model.add_node(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-5-2', input='conv-5-1-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-5-2-bn', input='conv-5-2')\n",
    "LV_loc_model.add_node(ELU(), name='conv-5-2-activ', input='conv-5-2-bn')\n",
    "LV_loc_model.add_node(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-5-3', input='conv-5-2-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='conv-5-3-bn', input='conv-5-3')\n",
    "LV_loc_model.add_node(ELU(), name='conv-5-3-activ', input='conv-5-3-bn')\n",
    "LV_loc_model.add_node(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'),\n",
    "               name='pool-5', input='conv-5-3-activ')\n",
    "\n",
    "LV_loc_model.add_node(Flatten(), name='flatten', input='pool-5')\n",
    "LV_loc_model.add_node(Dense(4096, activation='relu'), name='fc-1', input='flatten')\n",
    "LV_loc_model.add_node(Dropout(0.5), name='dropout-1', input='fc-1')\n",
    "LV_loc_model.add_node(Dense(4096, activation='relu'), name='fc-2', input='dropout-1')\n",
    "LV_loc_model.add_node(Dropout(0.5), name='dropout-2', input='fc-2')\n",
    "LV_loc_model.add_node(Reshape((64, 8, 8)), name='reshape', input='dropout-2')\n",
    "\n",
    "LV_loc_model.add_node(UpSampling2D(size=(2, 2), dim_ordering='th'), name='unpool-1', input='reshape')\n",
    "LV_loc_model.add_node(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-1-1', input='unpool-1')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-1-1-bn', input='deconv-1-1')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-1-1-activ', input='deconv-1-1-bn')\n",
    "LV_loc_model.add_node(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-1-2', input='deconv-1-1-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-1-2-bn', input='deconv-1-2')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-1-2-activ', input='deconv-1-2-bn')\n",
    "LV_loc_model.add_node(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-1-3', input='deconv-1-2-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-1-3-bn', input='deconv-1-3')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-1-3-activ', input='deconv-1-3-bn')\n",
    "\n",
    "LV_loc_model.add_node(UpSampling2D(size=(2, 2), dim_ordering='th'), name='unpool-2', input='deconv-1-3-activ')\n",
    "LV_loc_model.add_node(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-2-1', input='unpool-2')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-2-1-bn', input='deconv-2-1')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-2-1-activ', input='deconv-2-1-bn')\n",
    "LV_loc_model.add_node(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-2-2', input='deconv-2-1-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-2-2-bn', input='deconv-2-2')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-2-2-activ', input='deconv-2-2-bn')\n",
    "LV_loc_model.add_node(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-2-3', input='deconv-2-2-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-2-3-bn', input='deconv-2-3')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-2-3-activ', input='deconv-2-3-bn')\n",
    "\n",
    "LV_loc_model.add_node(UpSampling2D(size=(2, 2), dim_ordering='th'), name='unpool-3', input='deconv-2-3-activ')\n",
    "LV_loc_model.add_node(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-3-1', input='unpool-3')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-3-1-bn', input='deconv-3-1')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-3-1-activ', input='deconv-3-1-bn')\n",
    "LV_loc_model.add_node(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-3-2', input='deconv-3-1-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-3-2-bn', input='deconv-3-2')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-3-2-activ', input='deconv-3-2-bn')\n",
    "LV_loc_model.add_node(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-3-3', input='deconv-3-2-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-3-3-bn', input='deconv-3-3')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-3-3-activ', input='deconv-3-3-bn')\n",
    "\n",
    "LV_loc_model.add_node(UpSampling2D(size=(2, 2), dim_ordering='th'), name='unpool-4', input='deconv-3-3-activ')\n",
    "LV_loc_model.add_node(Convolution2D(64, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-4-1', input='unpool-4')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-4-1-bn', input='deconv-4-1')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-4-1-activ', input='deconv-4-1-bn')\n",
    "LV_loc_model.add_node(Convolution2D(64, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-4-2', input='deconv-4-1-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-4-2-bn', input='deconv-4-2')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-4-2-activ', input='deconv-4-2-bn')\n",
    "\n",
    "LV_loc_model.add_node(UpSampling2D(size=(2, 2), dim_ordering='th'), name='unpool-5', input='deconv-4-2-activ')\n",
    "LV_loc_model.add_node(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-5-1', input='unpool-5')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-5-1-bn', input='deconv-5-1')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-5-1-activ', input='deconv-5-1-bn')\n",
    "LV_loc_model.add_node(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-5-2', input='deconv-5-1-activ')\n",
    "LV_loc_model.add_node(BatchNormalization(), name='deconv-5-2-bn', input='deconv-5-2')\n",
    "LV_loc_model.add_node(ELU(), name='deconv-5-2-activ', input='deconv-5-2-bn')\n",
    "\n",
    "LV_loc_model.add_node(Convolution2D(1, 1, 1, activation='sigmoid', init='uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='prob-map', input='deconv-5-2-activ')\n",
    "LV_loc_model.add_node(Reshape((256, 256)), name='prob-map-reshape', input='prob-map')\n",
    "LV_loc_model.add_node(Dropout(0.5), name='prob-map-dropout', input='prob-map-reshape')\n",
    "\n",
    "LV_loc_model.add_node(GRU(256, activation='tanh', inner_activation='hard_sigmoid', return_sequences=True),\n",
    "               name='rnn-we', input='prob-map-dropout')\n",
    "LV_loc_model.add_node(GRU(256, activation='tanh', inner_activation='hard_sigmoid', go_backwards=True, return_sequences=True),\n",
    "               name='rnn-ew', input='prob-map-dropout')\n",
    "LV_loc_model.add_node(TimeDistributedDense(256, init='uniform', activation='sigmoid'),\n",
    "               name='rnn-1', inputs=['rnn-we', 'rnn-ew'], merge_mode='concat', concat_axis=-1)\n",
    "\n",
    "LV_loc_model.add_node(Rotate90(direction='counterclockwise'), name='rotate', input='prob-map-dropout')\n",
    "LV_loc_model.add_node(GRU(256, activation='tanh', inner_activation='hard_sigmoid', return_sequences=True),\n",
    "               name='rnn-ns', input='rotate')\n",
    "LV_loc_model.add_node(GRU(256, activation='tanh', inner_activation='hard_sigmoid', go_backwards=True, return_sequences=True),\n",
    "               name='rnn-sn', input='rotate')\n",
    "LV_loc_model.add_node(TimeDistributedDense(256, init='uniform', activation='sigmoid'),\n",
    "               name='rnn-2-rotated', inputs=['rnn-ns', 'rnn-sn'], merge_mode='concat', concat_axis=-1)\n",
    "LV_loc_model.add_node(Rotate90(direction='clockwise'), name='rnn-2', input='rnn-2-rotated')\n",
    "\n",
    "LV_loc_model.add_node(Activation('linear'), name='pre-output', inputs=['rnn-1', 'rnn-2'], merge_mode='mul')\n",
    "LV_loc_model.add_output(name='output', input='pre-output')\n",
    "\n",
    "LV_loc_model.compile('adam', {'output': binaryCE})\n",
    "\n",
    "\n",
    "def pred_loc_map(image_stack):\n",
    "    LV_loc_model.load_weights('../../model_weights/weights_trainset2_full.hdf5')\n",
    "    preds = LV_loc_model.predict({'input': np.expand_dims(np.array(image_stack).astype(np.float32), axis=1)}, \n",
    "                                 verbose=0)['output']\n",
    "    return [preds[i,:,:] for i in range(preds.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LV localized segmentation net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LV_seg_model = Graph()\n",
    "\n",
    "LV_seg_model.add_input(name='input', input_shape=(1, 96, 96))\n",
    "\n",
    "LV_seg_model.add_node(Convolution2D(64, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-1-1', input='input')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='conv-1-1-bn', input='conv-1-1')\n",
    "LV_seg_model.add_node(ELU(), name='conv-1-1-activ', input='conv-1-1-bn')\n",
    "LV_seg_model.add_node(Convolution2D(64, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-1-2', input='conv-1-1-activ')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='conv-1-2-bn', input='conv-1-2')\n",
    "LV_seg_model.add_node(ELU(), name='conv-1-2-activ', input='conv-1-2-bn')\n",
    "LV_seg_model.add_node(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'),\n",
    "               name='pool-1', input='conv-1-2-activ')\n",
    "\n",
    "LV_seg_model.add_node(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-2-1', input='pool-1')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='conv-2-1-bn', input='conv-2-1')\n",
    "LV_seg_model.add_node(ELU(), name='conv-2-1-activ', input='conv-2-1-bn')\n",
    "LV_seg_model.add_node(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-2-2', input='conv-2-1-activ')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='conv-2-2-bn', input='conv-2-2')\n",
    "LV_seg_model.add_node(ELU(), name='conv-2-2-activ', input='conv-2-2-bn')\n",
    "LV_seg_model.add_node(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'),\n",
    "               name='pool-2', input='conv-2-2-activ')\n",
    "\n",
    "LV_seg_model.add_node(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-3-1', input='pool-2')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='conv-3-1-bn', input='conv-3-1')\n",
    "LV_seg_model.add_node(ELU(), name='conv-3-1-activ', input='conv-3-1-bn')\n",
    "LV_seg_model.add_node(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-3-2', input='conv-3-1-activ')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='conv-3-2-bn', input='conv-3-2')\n",
    "LV_seg_model.add_node(ELU(), name='conv-3-2-activ', input='conv-3-2-bn')\n",
    "LV_seg_model.add_node(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-3-3', input='conv-3-2-activ')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='conv-3-3-bn', input='conv-3-3')\n",
    "LV_seg_model.add_node(ELU(), name='conv-3-3-activ', input='conv-3-3-bn')\n",
    "LV_seg_model.add_node(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'),\n",
    "               name='pool-3', input='conv-3-3-activ')\n",
    "\n",
    "LV_seg_model.add_node(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-4-1', input='pool-3')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='conv-4-1-bn', input='conv-4-1')\n",
    "LV_seg_model.add_node(ELU(), name='conv-4-1-activ', input='conv-4-1-bn')\n",
    "LV_seg_model.add_node(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-4-2', input='conv-4-1-activ')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='conv-4-2-bn', input='conv-4-2')\n",
    "LV_seg_model.add_node(ELU(), name='conv-4-2-activ', input='conv-4-2-bn')\n",
    "LV_seg_model.add_node(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='conv-4-3', input='conv-4-2-activ')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='conv-4-3-bn', input='conv-4-3')\n",
    "LV_seg_model.add_node(ELU(), name='conv-4-3-activ', input='conv-4-3-bn')\n",
    "LV_seg_model.add_node(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'),\n",
    "               name='pool-4', input='conv-4-3-activ')\n",
    "\n",
    "LV_seg_model.add_node(Flatten(), name='flatten', input='pool-4')\n",
    "LV_seg_model.add_node(Dense(2304, activation='relu'), name='fc-1', input='flatten')\n",
    "LV_seg_model.add_node(Dropout(0.5), name='dropout-1', input='fc-1')\n",
    "LV_seg_model.add_node(Dense(2304, activation='relu'), name='fc-2', input='dropout-1')\n",
    "LV_seg_model.add_node(Dropout(0.5), name='dropout-2', input='fc-2')\n",
    "LV_seg_model.add_node(Reshape((64, 6, 6)), name='reshape', input='dropout-2')\n",
    "\n",
    "LV_seg_model.add_node(UpSampling2D(size=(2, 2), dim_ordering='th'), name='unpool-1', input='reshape')\n",
    "LV_seg_model.add_node(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-1-1', input='unpool-1')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='deconv-1-1-bn', input='deconv-1-1')\n",
    "LV_seg_model.add_node(ELU(), name='deconv-1-1-activ', input='deconv-1-1-bn')\n",
    "LV_seg_model.add_node(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-1-2', input='deconv-1-1-activ')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='deconv-1-2-bn', input='deconv-1-2')\n",
    "LV_seg_model.add_node(ELU(), name='deconv-1-2-activ', input='deconv-1-2-bn')\n",
    "LV_seg_model.add_node(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-1-3', input='deconv-1-2-activ')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='deconv-1-3-bn', input='deconv-1-3')\n",
    "LV_seg_model.add_node(ELU(), name='deconv-1-3-activ', input='deconv-1-3-bn')\n",
    "\n",
    "LV_seg_model.add_node(UpSampling2D(size=(2, 2), dim_ordering='th'), name='unpool-2', input='deconv-1-3-activ')\n",
    "LV_seg_model.add_node(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-2-1', input='unpool-2')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='deconv-2-1-bn', input='deconv-2-1')\n",
    "LV_seg_model.add_node(ELU(), name='deconv-2-1-activ', input='deconv-2-1-bn')\n",
    "LV_seg_model.add_node(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-2-2', input='deconv-2-1-activ')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='deconv-2-2-bn', input='deconv-2-2')\n",
    "LV_seg_model.add_node(ELU(), name='deconv-2-2-activ', input='deconv-2-2-bn')\n",
    "LV_seg_model.add_node(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-2-3', input='deconv-2-2-activ')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='deconv-2-3-bn', input='deconv-2-3')\n",
    "LV_seg_model.add_node(ELU(), name='deconv-2-3-activ', input='deconv-2-3-bn')\n",
    "\n",
    "LV_seg_model.add_node(UpSampling2D(size=(2, 2), dim_ordering='th'), name='unpool-3', input='deconv-2-3-activ')\n",
    "LV_seg_model.add_node(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-3-1', input='unpool-3')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='deconv-3-1-bn', input='deconv-3-1')\n",
    "LV_seg_model.add_node(ELU(), name='deconv-3-1-activ', input='deconv-3-1-bn')\n",
    "LV_seg_model.add_node(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-3-2', input='deconv-3-1-activ')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='deconv-3-2-bn', input='deconv-3-2')\n",
    "LV_seg_model.add_node(ELU(), name='deconv-3-2-activ', input='deconv-3-2-bn')\n",
    "\n",
    "LV_seg_model.add_node(UpSampling2D(size=(2, 2), dim_ordering='th'), name='unpool-4', input='deconv-3-2-activ')\n",
    "LV_seg_model.add_node(Convolution2D(64, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-4-1', input='unpool-4')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='deconv-4-1-bn', input='deconv-4-1')\n",
    "LV_seg_model.add_node(ELU(), name='deconv-4-1-activ', input='deconv-4-1-bn')\n",
    "LV_seg_model.add_node(Convolution2D(64, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='deconv-4-2', input='deconv-4-1-activ')\n",
    "LV_seg_model.add_node(BatchNormalization(), name='deconv-4-2-bn', input='deconv-4-2')\n",
    "LV_seg_model.add_node(ELU(), name='deconv-4-2-activ', input='deconv-4-2-bn')\n",
    "\n",
    "LV_seg_model.add_node(Convolution2D(1, 1, 1, activation='sigmoid', init='uniform', border_mode='same', dim_ordering='th'),\n",
    "               name='prob-map', input='deconv-4-2-activ')\n",
    "LV_seg_model.add_node(Reshape((96, 96)), name='prob-map-reshape', input='prob-map')\n",
    "LV_seg_model.add_node(Dropout(0.5), name='prob-map-dropout', input='prob-map-reshape')\n",
    "\n",
    "LV_seg_model.add_node(GRU(96, activation='tanh', inner_activation='hard_sigmoid', return_sequences=True),\n",
    "               name='rnn-we', input='prob-map-dropout')\n",
    "LV_seg_model.add_node(GRU(96, activation='tanh', inner_activation='hard_sigmoid', go_backwards=True, return_sequences=True),\n",
    "               name='rnn-ew', input='prob-map-dropout')\n",
    "LV_seg_model.add_node(TimeDistributedDense(96, init='uniform', activation='sigmoid'),\n",
    "               name='rnn-1', inputs=['rnn-we', 'rnn-ew'], merge_mode='concat', concat_axis=-1)\n",
    "\n",
    "LV_seg_model.add_node(Rotate90(direction='counterclockwise'), name='rotate', input='prob-map-dropout')\n",
    "LV_seg_model.add_node(GRU(96, activation='tanh', inner_activation='hard_sigmoid', return_sequences=True),\n",
    "               name='rnn-ns', input='rotate')\n",
    "LV_seg_model.add_node(GRU(96, activation='tanh', inner_activation='hard_sigmoid', go_backwards=True, return_sequences=True),\n",
    "               name='rnn-sn', input='rotate')\n",
    "LV_seg_model.add_node(TimeDistributedDense(96, init='uniform', activation='sigmoid'),\n",
    "               name='rnn-2-rotated', inputs=['rnn-ns', 'rnn-sn'], merge_mode='concat', concat_axis=-1)\n",
    "LV_seg_model.add_node(Rotate90(direction='clockwise'), name='rnn-2', input='rnn-2-rotated')\n",
    "\n",
    "LV_seg_model.add_node(Activation('linear'), name='pre-output', inputs=['rnn-1', 'rnn-2'], merge_mode='mul')\n",
    "LV_seg_model.add_output(name='output', input='pre-output')\n",
    "\n",
    "LV_seg_model.compile('adam', {'output': binaryCE})\n",
    "\n",
    "\n",
    "def pred_seg_map(image_stack):\n",
    "    LV_seg_model.load_weights('../../model_weights/weights_trainset2_local.hdf5')\n",
    "    preds = LV_seg_model.predict({'input': np.expand_dims(np.array(image_stack).astype(np.float32), axis=1)}, \n",
    "                                  verbose=0)['output']\n",
    "    return [preds[i,:,:] for i in range(preds.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### segmentation optimal threshold networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresh_optimizer_ED = Sequential()\n",
    "\n",
    "thresh_optimizer_ED.add(Convolution2D(64, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th', \n",
    "                                      input_shape=(1, 96, 96)))\n",
    "thresh_optimizer_ED.add(BatchNormalization())\n",
    "thresh_optimizer_ED.add(ELU())\n",
    "thresh_optimizer_ED.add(Convolution2D(64, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ED.add(BatchNormalization())\n",
    "thresh_optimizer_ED.add(ELU())\n",
    "thresh_optimizer_ED.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), border_mode='valid', dim_ordering='th'))\n",
    "\n",
    "thresh_optimizer_ED.add(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ED.add(BatchNormalization())\n",
    "thresh_optimizer_ED.add(ELU())\n",
    "thresh_optimizer_ED.add(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ED.add(BatchNormalization())\n",
    "thresh_optimizer_ED.add(ELU())\n",
    "thresh_optimizer_ED.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), border_mode='valid', dim_ordering='th'))\n",
    "\n",
    "thresh_optimizer_ED.add(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ED.add(BatchNormalization())\n",
    "thresh_optimizer_ED.add(ELU())\n",
    "thresh_optimizer_ED.add(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ED.add(BatchNormalization())\n",
    "thresh_optimizer_ED.add(ELU())\n",
    "thresh_optimizer_ED.add(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ED.add(BatchNormalization())\n",
    "thresh_optimizer_ED.add(ELU())\n",
    "thresh_optimizer_ED.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), border_mode='valid', dim_ordering='th'))\n",
    "\n",
    "thresh_optimizer_ED.add(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ED.add(BatchNormalization())\n",
    "thresh_optimizer_ED.add(ELU())\n",
    "thresh_optimizer_ED.add(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ED.add(BatchNormalization())\n",
    "thresh_optimizer_ED.add(ELU())\n",
    "thresh_optimizer_ED.add(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ED.add(BatchNormalization())\n",
    "thresh_optimizer_ED.add(ELU())\n",
    "thresh_optimizer_ED.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), border_mode='valid', dim_ordering='th'))\n",
    "\n",
    "thresh_optimizer_ED.add(Flatten())\n",
    "thresh_optimizer_ED.add(Dense(1024, activation='relu'))\n",
    "thresh_optimizer_ED.add(Dropout(0.5))\n",
    "thresh_optimizer_ED.add(Dense(1, activation='linear'))\n",
    "\n",
    "thresh_optimizer_ED.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "thresh_optimizer_ED.load_weights('../../model_weights/weights_trainset2and3_thresh_optimizer_ED.hdf5')\n",
    "\n",
    "\n",
    "def get_optimal_thresh_ED(image_stack):\n",
    "    preds = thresh_optimizer_ED.predict(np.expand_dims(np.array(image_stack).astype(np.float32), axis=1), verbose=0)\n",
    "    return np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresh_optimizer_ES = Sequential()\n",
    "\n",
    "thresh_optimizer_ES.add(Convolution2D(64, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th', \n",
    "                                      input_shape=(1, 96, 96)))\n",
    "thresh_optimizer_ES.add(BatchNormalization())\n",
    "thresh_optimizer_ES.add(ELU())\n",
    "thresh_optimizer_ES.add(Convolution2D(64, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ES.add(BatchNormalization())\n",
    "thresh_optimizer_ES.add(ELU())\n",
    "thresh_optimizer_ES.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), border_mode='valid', dim_ordering='th'))\n",
    "\n",
    "thresh_optimizer_ES.add(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ES.add(BatchNormalization())\n",
    "thresh_optimizer_ES.add(ELU())\n",
    "thresh_optimizer_ES.add(Convolution2D(128, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ES.add(BatchNormalization())\n",
    "thresh_optimizer_ES.add(ELU())\n",
    "thresh_optimizer_ES.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), border_mode='valid', dim_ordering='th'))\n",
    "\n",
    "thresh_optimizer_ES.add(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ES.add(BatchNormalization())\n",
    "thresh_optimizer_ES.add(ELU())\n",
    "thresh_optimizer_ES.add(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ES.add(BatchNormalization())\n",
    "thresh_optimizer_ES.add(ELU())\n",
    "thresh_optimizer_ES.add(Convolution2D(256, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ES.add(BatchNormalization())\n",
    "thresh_optimizer_ES.add(ELU())\n",
    "thresh_optimizer_ES.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), border_mode='valid', dim_ordering='th'))\n",
    "\n",
    "thresh_optimizer_ES.add(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ES.add(BatchNormalization())\n",
    "thresh_optimizer_ES.add(ELU())\n",
    "thresh_optimizer_ES.add(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ES.add(BatchNormalization())\n",
    "thresh_optimizer_ES.add(ELU())\n",
    "thresh_optimizer_ES.add(Convolution2D(512, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "thresh_optimizer_ES.add(BatchNormalization())\n",
    "thresh_optimizer_ES.add(ELU())\n",
    "thresh_optimizer_ES.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), border_mode='valid', dim_ordering='th'))\n",
    "\n",
    "thresh_optimizer_ES.add(Flatten())\n",
    "thresh_optimizer_ES.add(Dense(1024, activation='relu'))\n",
    "thresh_optimizer_ES.add(Dropout(0.5))\n",
    "thresh_optimizer_ES.add(Dense(1, activation='linear'))\n",
    "\n",
    "thresh_optimizer_ES.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "thresh_optimizer_ES.load_weights('../../model_weights/weights_trainset2and3_thresh_optimizer_ES.hdf5')\n",
    "\n",
    "\n",
    "def get_optimal_thresh_ES(image_stack):\n",
    "    preds = thresh_optimizer_ES.predict(np.expand_dims(np.array(image_stack).astype(np.float32), axis=1), verbose=0)\n",
    "    return np.mean(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data functions for volume nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_MIP(filepaths, full_size=256, frame=0):\n",
    "    series_filepaths_all = get_all_series_filepaths(filepaths)\n",
    "\n",
    "    vol3d = []\n",
    "    for series_filepaths in series_filepaths_all:\n",
    "        fname, fpath = natsorted(series_filepaths, lambda x: x[0])[frame]\n",
    "        df = dicom.read_file(fpath)\n",
    "        img2d = df.pixel_array\n",
    "        vol3d.append(apply_per_slice_norm(crop_to_square(img2d, full_size)).astype(np.float32))\n",
    "    orig_shape = img2d.shape\n",
    "    pixel_spacing = df.PixelSpacing\n",
    "    vol3d_mask = pred_loc_map(vol3d)\n",
    "\n",
    "    vol3d_MIP = np.mean(np.array(vol3d), axis=0)\n",
    "    vol3d_mask_MIP = np.mean(np.array(vol3d_mask), axis=0)\n",
    "\n",
    "    return vol3d_MIP, vol3d_mask_MIP, orig_shape, pixel_spacing\n",
    "\n",
    "\n",
    "def get_MIP_centroid(vol3d_mask_MIP):\n",
    "    return ndimage.measurements.center_of_mass(vol3d_mask_MIP)\n",
    "\n",
    "\n",
    "def create_localized_image_stack(filepaths, centroid, full_size=256, local_size=96, frame=0):\n",
    "    series_filepaths_all = get_all_series_filepaths(filepaths)\n",
    "                        \n",
    "    # sort series by z-locations\n",
    "    z_locs = []\n",
    "    for series_filepaths in series_filepaths_all:\n",
    "        df = dicom.read_file(natsorted(series_filepaths, lambda x: x[0])[frame][1])\n",
    "        z_locs.append(normalized_z_loc(df))\n",
    "    series_filepaths_all_zsorted = sorted(zip([min(z_locs) - z_loc for z_loc in z_locs], \n",
    "                                              series_filepaths_all), key=lambda pair: pair[0])\n",
    "    \n",
    "    series_filepaths_all_zsorted_with_depths = []\n",
    "    for i in range(len(series_filepaths_all_zsorted)):\n",
    "        z_loc, series_filepaths = series_filepaths_all_zsorted[i]\n",
    "        if i == len(series_filepaths_all_zsorted) - 1:\n",
    "            z_depth = float(dicom.read_file(natsorted(series_filepaths, lambda x: x[0])[frame][1]).SliceThickness)\n",
    "        else:\n",
    "            z_depth = series_filepaths_all_zsorted[i+1][0] - z_loc\n",
    "\n",
    "        # filter out tiny depths, which are likely repeats\n",
    "        if z_depth > 0.01:\n",
    "            series_filepaths_all_zsorted_with_depths.append((z_depth, series_filepaths))\n",
    "\n",
    "    img_stack = []\n",
    "    z_depths = []\n",
    "    for z_depth, series_filepaths in series_filepaths_all_zsorted_with_depths:\n",
    "        fname, fpath = natsorted(series_filepaths, lambda x: x[0])[frame]\n",
    "        df = dicom.read_file(fpath)\n",
    "        img = df.pixel_array\n",
    "        img_localized, _, _ = localize_to_centroid(crop_to_square(img, full_size), centroid, local_size)\n",
    "        img_processed = apply_per_slice_norm(img_localized)\n",
    "        img_stack.append(img_processed.astype(np.float32))\n",
    "        z_depths.append(z_depth)\n",
    "    orig_shape = img.shape\n",
    "    pixel_spacing = [float(s) for s in df.PixelSpacing]\n",
    "    \n",
    "    img_stack_masks = pred_seg_map([apply_per_slice_norm(img) for img in img_stack])\n",
    "\n",
    "    return img_stack, img_stack_masks, z_depths, orig_shape, pixel_spacing\n",
    "\n",
    "\n",
    "def create_full_image_stack(filepaths, full_size=256, frame=0):\n",
    "    series_filepaths_all = get_all_series_filepaths(filepaths)\n",
    "                        \n",
    "    # sort series by z-locations\n",
    "    z_locs = []\n",
    "    for series_filepaths in series_filepaths_all:\n",
    "        df = dicom.read_file(natsorted(series_filepaths, lambda x: x[0])[frame][1])\n",
    "        z_locs.append(normalized_z_loc(df))\n",
    "    series_filepaths_all_zsorted = sorted(zip([min(z_locs) - z_loc - min(z_locs) for z_loc in z_locs], \n",
    "                                              series_filepaths_all), key=lambda pair: pair[0])\n",
    "    \n",
    "    series_filepaths_all_zsorted_with_depths = []\n",
    "    for i in range(len(series_filepaths_all_zsorted)):\n",
    "        z_loc, series_filepaths = series_filepaths_all_zsorted[i]\n",
    "        if i == len(series_filepaths_all_zsorted) - 1:\n",
    "            z_depth = float(dicom.read_file(natsorted(series_filepaths, lambda x: x[0])[frame][1]).SliceThickness)\n",
    "        else:\n",
    "            z_depth = series_filepaths_all_zsorted[i+1][0] - z_loc\n",
    "\n",
    "        # filter out tiny depths, which are likely repeats\n",
    "        if z_depth > 0.01:\n",
    "            series_filepaths_all_zsorted_with_depths.append((z_depth, series_filepaths))\n",
    "\n",
    "    img_stack = []\n",
    "    z_depths = []\n",
    "    for z_depth, series_filepaths in series_filepaths_all_zsorted_with_depths:\n",
    "        fname, fpath = natsorted(series_filepaths, lambda x: x[0])[frame]\n",
    "        df = dicom.read_file(fpath)\n",
    "        img = df.pixel_array\n",
    "        img_processed = apply_per_slice_norm(crop_to_square(img, full_size))\n",
    "        img_stack.append(img_processed.astype(np.float32))\n",
    "        z_depths.append(z_depth)\n",
    "    orig_shape = img.shape\n",
    "    pixel_spacing = [float(s) for s in df.PixelSpacing]\n",
    "    \n",
    "    #img_stack_masks = pred_loc_map(img_stack)\n",
    "    img_stack_masks = None\n",
    "\n",
    "    return img_stack, img_stack_masks, z_depths, orig_shape, pixel_spacing\n",
    "\n",
    "\n",
    "def calculate_volume(img_stack_masks, threshold, z_depths, pixel_spacing, orig_shape):\n",
    "    volume = 0\n",
    "    for i in range(len(img_stack_masks)):\n",
    "        volume += np.sum(img_stack_masks[i] >= threshold) * (z_depths[i] / 10) * \\\n",
    "            (pixel_spacing[0] / 10) * (pixel_spacing[1] / 10) * \\\n",
    "            (min(orig_shape) / 256) * (min(orig_shape) / 256)\n",
    "    return volume\n",
    "\n",
    "\n",
    "def get_start_end(img_stack_local, local_top_exclusion, local_apex_exclusion):\n",
    "    start = 0\n",
    "    end = len(img_stack_local)\n",
    "    for i in range(0, len(img_stack_local)//2):\n",
    "        if local_top_exclusion[i] == 0:\n",
    "            break\n",
    "        else:\n",
    "            start = i\n",
    "    for i in range(len(img_stack_local)//2, len(img_stack_local))[::-1]:\n",
    "        if local_apex_exclusion[i] == 0:\n",
    "            break\n",
    "        else:\n",
    "            end = i+1\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def create_distribution(volumes, probabilities):\n",
    "    dist = np.zeros(600)\n",
    "    for i in range(1, len(probabilities)-2):\n",
    "        start = int(round(volumes[i]))\n",
    "        end = int(round(volumes[i+1]))\n",
    "        dist[start:end] = probabilities[i]\n",
    "    dist[end:601] = 1\n",
    "    return dist\n",
    "\n",
    "\n",
    "from scipy.special import erf\n",
    "def create_CDF(mean, stddev):\n",
    "    return 0.5 + 0.5 * erf((np.arange(600) - mean)/(stddev * np.sqrt(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test localization, segmentation, z-slice exclusion, combined volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for pt in range(1, 2):\n",
    "\n",
    "    filepaths = filepaths_test[pt]\n",
    "    vol3d_MIP, vol3d_mask_MIP, orig_shape, pixel_spacing = \\\n",
    "        create_MIP(filepaths, full_size=256, frame=0)\n",
    "    centroid = get_MIP_centroid(vol3d_mask_MIP)\n",
    "    \n",
    "    img_stack, img_stack_masks, z_depths, orig_shape, pixel_spacing = \\\n",
    "        create_localized_image_stack(filepaths, centroid, full_size=256, local_size=96, frame=0)\n",
    "    start, end = 1, len(img_stack)\n",
    "    optimal_thresh = get_optimal_thresh_ED(img_stack[start:end])\n",
    "    volume = calculate_volume(img_stack_masks[start:end], optimal_thresh, \n",
    "                              z_depths[start:end], pixel_spacing, orig_shape)\n",
    "\n",
    "    plt.figure(figsize=((20,4)))\n",
    "    plt.suptitle('pred: {} true: {}'.format(volume, diastole_labels[pt]))\n",
    "    for i in range(len(img_stack)):\n",
    "        plt.subplot(3, len(img_stack), i+1)\n",
    "        plt.imshow(img_stack[i], cmap=plt.cm.magma)\n",
    "    for i in range(len(img_stack)):\n",
    "        plt.subplot(3, len(img_stack), len(img_stack)+i+1)\n",
    "        plt.imshow(img_stack_masks[i], cmap=plt.cm.hot)\n",
    "    for i in range(len(img_stack)):\n",
    "        plt.subplot(3, len(img_stack), 2*len(img_stack)+i+1)\n",
    "        plt.imshow(img_stack[i] * img_stack_masks[i], cmap=plt.cm.magma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for pt in range(1, 2):\n",
    "\n",
    "    filepaths = filepaths_test[pt]\n",
    "    vol3d_MIP, vol3d_mask_MIP, orig_shape, pixel_spacing = \\\n",
    "        create_MIP(filepaths, full_size=256, frame=pt_es_frame[pt])\n",
    "    centroid = get_MIP_centroid(vol3d_mask_MIP)\n",
    "    \n",
    "    img_stack, img_stack_masks, z_depths, orig_shape, pixel_spacing = \\\n",
    "        create_localized_image_stack(filepaths, centroid, full_size=256, local_size=96, frame=pt_es_frame[pt])\n",
    "    start, end = 2, -1\n",
    "    optimal_thresh = get_optimal_thresh_ES(img_stack[start:end])\n",
    "    volume = calculate_volume(img_stack_masks[start:end], optimal_thresh, \n",
    "                              z_depths[start:end], pixel_spacing, orig_shape)\n",
    "\n",
    "    plt.figure(figsize=((20,4)))\n",
    "    plt.suptitle('pred: {} true: {}'.format(volume, systole_labels[pt]))\n",
    "    for i in range(len(img_stack)):\n",
    "        plt.subplot(3, len(img_stack), i+1)\n",
    "        plt.imshow(img_stack[i], cmap=plt.cm.magma)\n",
    "    for i in range(len(img_stack)):\n",
    "        plt.subplot(3, len(img_stack), len(img_stack)+i+1)\n",
    "        plt.imshow(img_stack_masks[i], cmap=plt.cm.hot)\n",
    "    for i in range(len(img_stack)):\n",
    "        plt.subplot(3, len(img_stack), 2*len(img_stack)+i+1)\n",
    "        plt.imshow(img_stack[i] * img_stack_masks[i], cmap=plt.cm.magma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140...done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "test_volumes_pred_ED = []\n",
    "\n",
    "for pt in tqdm(range(701, 1141)):\n",
    "    \n",
    "    filepaths = filepaths_test[pt]\n",
    "    frames = [28,29,0,1,2]\n",
    "    \n",
    "    frame_volumes = []\n",
    "    for frame in frames:\n",
    "        vol3d_MIP, vol3d_mask_MIP, orig_shape, pixel_spacing = \\\n",
    "            create_MIP(filepaths, full_size=256, frame=frame)\n",
    "        centroid = get_MIP_centroid(vol3d_mask_MIP)\n",
    "\n",
    "        img_stack, img_stack_masks, z_depths, orig_shape, pixel_spacing = \\\n",
    "            create_localized_image_stack(filepaths, centroid, full_size=256, local_size=96, frame=frame)\n",
    "        start, end = 1, len(img_stack)\n",
    "        optimal_thresh = get_optimal_thresh_ED(img_stack)\n",
    "        volume = calculate_volume(img_stack_masks[start:end], optimal_thresh, \n",
    "                                  z_depths[start:end], pixel_spacing, orig_shape)\n",
    "        frame_volumes.append(volume)\n",
    "\n",
    "    test_volumes_pred_ED.append(np.max(frame_volumes))\n",
    "print(pt, end='...done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../../data_supp/pt_es_frame_test.pkl', 'rb') as f:\n",
    "    pt_es_frame_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140...done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "test_volumes_pred_ES = []\n",
    "\n",
    "for pt in tqdm(range(701, 1141)):\n",
    "        \n",
    "    filepaths = filepaths_test[pt]\n",
    "    frames = [(pt_es_frame_test[pt] + i) for i in range(-2,3)]\n",
    "    \n",
    "    frame_volumes = []\n",
    "    for frame in frames:\n",
    "        vol3d_MIP, vol3d_mask_MIP, orig_shape, pixel_spacing = \\\n",
    "            create_MIP(filepaths, full_size=256, frame=frame)\n",
    "        centroid = get_MIP_centroid(vol3d_mask_MIP)\n",
    "\n",
    "        img_stack, img_stack_masks, z_depths, orig_shape, pixel_spacing = \\\n",
    "            create_localized_image_stack(filepaths, centroid, full_size=256, local_size=96, frame=frame)\n",
    "        start, end = 2, -1\n",
    "        optimal_thresh = get_optimal_thresh_ES(img_stack)\n",
    "        volume = calculate_volume(img_stack_masks[start:end], optimal_thresh, \n",
    "                                  z_depths[start:end], pixel_spacing, orig_shape)\n",
    "        frame_volumes.append(volume)\n",
    "\n",
    "    test_volumes_pred_ES.append(np.min(frame_volumes))\n",
    "print(pt, end='...done.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "test_pt_indices = dict([(pt,i) for i,pt in enumerate(range(701, 1141))])\n",
    "\n",
    "test_volumes_pred_ED_dist = [create_distribution([vol + 33*(i-50)/100.0 for i in range(101)], \n",
    "                                                [i/100.0 for i in range(101)]) \n",
    "                            for vol in test_volumes_pred_ED]\n",
    "\n",
    "test_volumes_pred_ES_dist = [create_distribution([vol + 28*(i-50)/100.0 for i in range(101)], \n",
    "                                                [i/100.0 for i in range(101)]) \n",
    "                            for vol in test_volumes_pred_ES]\n",
    "\n",
    "with open('../../data/sample_submission_test.csv', 'r') as fi:\n",
    "    reader = csv.reader(fi)\n",
    "    header = next(reader)\n",
    "    \n",
    "    with open('../../submissions/test-3.csv', 'w') as fo:\n",
    "        writer = csv.writer(fo, lineterminator='\\n')\n",
    "        writer.writerow(header)\n",
    "        for rowin in tqdm(reader):\n",
    "            _id = rowin[0]\n",
    "            pt, mode = _id.split('_')\n",
    "            rowout = [_id]\n",
    "            idx = test_pt_indices[int(pt)]\n",
    "            if mode.lower() == 'systole':\n",
    "                rowout.extend(test_volumes_pred_ES_dist[idx].tolist())\n",
    "            elif mode.lower() == 'diastole':\n",
    "                rowout.extend(test_volumes_pred_ED_dist[idx].tolist())\n",
    "            else:\n",
    "                raise\n",
    "            writer.writerow(rowout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
