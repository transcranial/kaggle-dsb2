{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import dicom\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from tinydb import TinyDB, Query\n",
    "from natsort import natsorted\n",
    "from skimage import transform\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load and shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(pts_train, pts_train_val, \n",
    " data_train, data_diffs_train, label_sys_train, label_dia_train, \n",
    " data_train_val, data_diffs_train_val, label_sys_train_val, label_dia_train_val, \n",
    " data_val, data_diffs_val, data_val_pt_index) = joblib.load('../data_proc/0-data_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle_index = list(range(data_train.shape[0]))\n",
    "random.shuffle(shuffle_index)\n",
    "data_train = data_train[shuffle_index]\n",
    "label_sys_train = label_sys_train[shuffle_index]\n",
    "label_dia_train = label_dia_train[shuffle_index]\n",
    "\n",
    "shuffle_index = list(range(data_train_val.shape[0]))\n",
    "random.shuffle(shuffle_index)\n",
    "data_train_val = data_train_val[shuffle_index]\n",
    "label_sys_train_val = label_sys_train_val[shuffle_index]\n",
    "label_dia_train_val = label_dia_train_val[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 3: Tesla K80 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Graph\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten, Merge, Reshape, Lambda\n",
    "from keras.layers.core import TimeDistributedDense, TimeDistributedMerge\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ParametricSoftplus, ELU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.noise import GaussianDropout, GaussianNoise\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous Ranked Probability Score (used as loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from theano import tensor as T\n",
    "\n",
    "# currently for theano, would need modifications for tensorflow\n",
    "\n",
    "def CRPS(y_true, y_pred):\n",
    "    return K.mean(K.square(T.cumsum(y_pred, axis=-1) - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_sys = Sequential()\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th', input_shape=(30, 196, 196)))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(ELU())\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(ELU())\n",
    "model_sys.add(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(ELU())\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(ELU())\n",
    "model_sys.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(ELU())\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(ELU())\n",
    "model_sys.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(ELU())\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(ELU())\n",
    "model_sys.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_sys.add(Flatten())\n",
    "model_sys.add(Dense(1024, activation='relu'))\n",
    "model_sys.add(Dropout(0.5))\n",
    "model_sys.add(Dense(600))\n",
    "model_sys.add(Activation('softmax'))\n",
    "\n",
    "model_sys.compile(loss=CRPS, optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5065 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      "93s - loss: 0.1188 - val_loss: 0.0324\n",
      "Epoch 00000: val_loss improved from inf to 0.03237, saving model to ../model_weights/0-1-convnet_basic_systole.hdf5\n",
      "Epoch 2/100\n",
      "93s - loss: 0.0379 - val_loss: 0.0337\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 3/100\n",
      "93s - loss: 0.0370 - val_loss: 0.0328\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 4/100\n",
      "93s - loss: 0.0365 - val_loss: 0.0316\n",
      "Epoch 00003: val_loss improved from 0.03237 to 0.03164, saving model to ../model_weights/0-1-convnet_basic_systole.hdf5\n",
      "Epoch 5/100\n",
      "93s - loss: 0.0360 - val_loss: 0.0310\n",
      "Epoch 00004: val_loss improved from 0.03164 to 0.03104, saving model to ../model_weights/0-1-convnet_basic_systole.hdf5\n",
      "Epoch 6/100\n",
      "93s - loss: 0.0355 - val_loss: 0.0316\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 7/100\n",
      "93s - loss: 0.0352 - val_loss: 0.0302\n",
      "Epoch 00006: val_loss improved from 0.03104 to 0.03025, saving model to ../model_weights/0-1-convnet_basic_systole.hdf5\n",
      "Epoch 8/100\n",
      "93s - loss: 0.0347 - val_loss: 0.0306\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 9/100\n",
      "93s - loss: 0.0344 - val_loss: 0.0299\n",
      "Epoch 00008: val_loss improved from 0.03025 to 0.02986, saving model to ../model_weights/0-1-convnet_basic_systole.hdf5\n",
      "Epoch 10/100\n",
      "93s - loss: 0.0338 - val_loss: 0.0308\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 11/100\n",
      "93s - loss: 0.0331 - val_loss: 0.0300\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 12/100\n",
      "93s - loss: 0.0325 - val_loss: 0.0290\n",
      "Epoch 00011: val_loss improved from 0.02986 to 0.02897, saving model to ../model_weights/0-1-convnet_basic_systole.hdf5\n",
      "Epoch 13/100\n",
      "93s - loss: 0.0314 - val_loss: 0.0280\n",
      "Epoch 00012: val_loss improved from 0.02897 to 0.02802, saving model to ../model_weights/0-1-convnet_basic_systole.hdf5\n",
      "Epoch 14/100\n",
      "93s - loss: 0.0306 - val_loss: 0.0287\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 15/100\n",
      "93s - loss: 0.0295 - val_loss: 0.0301\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 16/100\n",
      "93s - loss: 0.0285 - val_loss: 0.0303\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3ffbc8160>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 100\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath='../model_weights/0-1-convnet_basic_systole.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "\n",
    "model_sys.fit(data_train, label_sys_train, \n",
    "              batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2,\n",
    "              validation_data=(data_train_val, label_sys_train_val), shuffle=True,\n",
    "              callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dia = Sequential()\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th', input_shape=(30, 196, 196)))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(ELU())\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(ELU())\n",
    "model_dia.add(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(ELU())\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(ELU())\n",
    "model_dia.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(ELU())\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(ELU())\n",
    "model_dia.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(ELU())\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(ELU())\n",
    "model_dia.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_dia.add(Flatten())\n",
    "model_dia.add(Dense(1024, activation='relu'))\n",
    "model_dia.add(Dropout(0.5))\n",
    "model_dia.add(Dense(600))\n",
    "model_dia.add(Activation('softmax'))\n",
    "\n",
    "model_dia.compile(loss=CRPS, optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5065 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      "93s - loss: 0.1108 - val_loss: 0.0598\n",
      "Epoch 00000: val_loss improved from inf to 0.05984, saving model to ../model_weights/0-1-convnet_basic_diastole.hdf5\n",
      "Epoch 2/100\n",
      "93s - loss: 0.0587 - val_loss: 0.0498\n",
      "Epoch 00001: val_loss improved from 0.05984 to 0.04984, saving model to ../model_weights/0-1-convnet_basic_diastole.hdf5\n",
      "Epoch 3/100\n",
      "93s - loss: 0.0552 - val_loss: 0.0489\n",
      "Epoch 00002: val_loss improved from 0.04984 to 0.04890, saving model to ../model_weights/0-1-convnet_basic_diastole.hdf5\n",
      "Epoch 4/100\n",
      "93s - loss: 0.0548 - val_loss: 0.0473\n",
      "Epoch 00003: val_loss improved from 0.04890 to 0.04729, saving model to ../model_weights/0-1-convnet_basic_diastole.hdf5\n",
      "Epoch 5/100\n",
      "93s - loss: 0.0543 - val_loss: 0.0469\n",
      "Epoch 00004: val_loss improved from 0.04729 to 0.04686, saving model to ../model_weights/0-1-convnet_basic_diastole.hdf5\n",
      "Epoch 6/100\n",
      "93s - loss: 0.0538 - val_loss: 0.0460\n",
      "Epoch 00005: val_loss improved from 0.04686 to 0.04601, saving model to ../model_weights/0-1-convnet_basic_diastole.hdf5\n",
      "Epoch 7/100\n",
      "93s - loss: 0.0533 - val_loss: 0.0449\n",
      "Epoch 00006: val_loss improved from 0.04601 to 0.04495, saving model to ../model_weights/0-1-convnet_basic_diastole.hdf5\n",
      "Epoch 8/100\n",
      "93s - loss: 0.0526 - val_loss: 0.0450\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 9/100\n",
      "93s - loss: 0.0517 - val_loss: 0.0441\n",
      "Epoch 00008: val_loss improved from 0.04495 to 0.04413, saving model to ../model_weights/0-1-convnet_basic_diastole.hdf5\n",
      "Epoch 10/100\n",
      "93s - loss: 0.0504 - val_loss: 0.0469\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 11/100\n",
      "93s - loss: 0.0486 - val_loss: 0.0422\n",
      "Epoch 00010: val_loss improved from 0.04413 to 0.04223, saving model to ../model_weights/0-1-convnet_basic_diastole.hdf5\n",
      "Epoch 12/100\n",
      "93s - loss: 0.0469 - val_loss: 0.0461\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 13/100\n",
      "93s - loss: 0.0457 - val_loss: 0.0397\n",
      "Epoch 00012: val_loss improved from 0.04223 to 0.03966, saving model to ../model_weights/0-1-convnet_basic_diastole.hdf5\n",
      "Epoch 14/100\n",
      "93s - loss: 0.0438 - val_loss: 0.0431\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 15/100\n",
      "93s - loss: 0.0426 - val_loss: 0.0392\n",
      "Epoch 00014: val_loss improved from 0.03966 to 0.03922, saving model to ../model_weights/0-1-convnet_basic_diastole.hdf5\n",
      "Epoch 16/100\n",
      "93s - loss: 0.0419 - val_loss: 0.0441\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 17/100\n",
      "93s - loss: 0.0402 - val_loss: 0.0406\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 18/100\n",
      "93s - loss: 0.0393 - val_loss: 0.0556\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3f09dfc88>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 100\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath='../model_weights/0-1-convnet_basic_diastole.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "\n",
    "model_dia.fit(data_train, label_dia_train, \n",
    "              batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2,\n",
    "              validation_data=(data_train_val, label_dia_train_val), shuffle=True,\n",
    "              callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_sys.load_weights('../model_weights/0-1-convnet_basic_systole.hdf5')\n",
    "model_dia.load_weights('../model_weights/0-1-convnet_basic_diastole.hdf5')\n",
    "\n",
    "preds_sys = model_sys.predict(data_val, verbose=0)\n",
    "preds_dia = model_dia.predict(data_val, verbose=0)\n",
    "\n",
    "preds_sys = np.clip(np.cumsum(preds_sys, axis=-1), 0, 1)\n",
    "preds_dia = np.clip(np.cumsum(preds_dia, axis=-1), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_sys_pt = {}\n",
    "preds_dia_pt = {}\n",
    "for pt in range(501, 701):\n",
    "    preds_sys_pt[pt] = np.mean(preds_sys[np.where(data_val_pt_index == pt)[0]], axis=0)\n",
    "    preds_dia_pt[pt] = np.mean(preds_dia[np.where(data_val_pt_index == pt)[0]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "with open('../data/sample_submission_validate.csv', 'r') as fi:\n",
    "    reader = csv.reader(fi)\n",
    "    header = next(reader)\n",
    "    \n",
    "    with open('../submissions/0-1-convnet_basic.csv', 'w') as fo:\n",
    "        writer = csv.writer(fo, lineterminator='\\n')\n",
    "        writer.writerow(header)\n",
    "        for rowin in tqdm(reader):\n",
    "            _id = rowin[0]\n",
    "            pt, mode = _id.split('_')\n",
    "            rowout = [_id]\n",
    "            if mode.lower() == 'systole':\n",
    "                rowout.extend(preds_sys_pt[int(pt)].tolist())\n",
    "            elif mode.lower() == 'diastole':\n",
    "                rowout.extend(preds_dia_pt[int(pt)].tolist())\n",
    "            else:\n",
    "                raise\n",
    "            writer.writerow(rowout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
