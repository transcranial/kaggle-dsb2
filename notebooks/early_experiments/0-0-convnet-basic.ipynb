{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import dicom\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from tinydb import TinyDB, Query\n",
    "from natsort import natsorted\n",
    "from skimage import transform\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load and shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(pts_train, pts_train_val, \n",
    " data_train, data_diffs_train, label_sys_train, label_dia_train, \n",
    " data_train_val, data_diffs_train_val, label_sys_train_val, label_dia_train_val, \n",
    " data_val, data_diffs_val, data_val_pt_index) = joblib.load('../data_proc/0-data_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle_index = list(range(data_train.shape[0]))\n",
    "random.shuffle(shuffle_index)\n",
    "data_train = data_train[shuffle_index]\n",
    "label_sys_train = label_sys_train[shuffle_index]\n",
    "label_dia_train = label_dia_train[shuffle_index]\n",
    "\n",
    "shuffle_index = list(range(data_train_val.shape[0]))\n",
    "random.shuffle(shuffle_index)\n",
    "data_train_val = data_train_val[shuffle_index]\n",
    "label_sys_train_val = label_sys_train_val[shuffle_index]\n",
    "label_dia_train_val = label_dia_train_val[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_invalid_cdf(preds):\n",
    "    problematic_i = []\n",
    "    for i in range(preds.shape[0]):\n",
    "        if np.min(preds[i,:]) < 0 or np.max(preds[i,:]) > 1:\n",
    "            problematic_i.append(i)\n",
    "            continue\n",
    "        for j in range(preds.shape[1]-1):\n",
    "            if preds[i,j] > preds[i,j+1]:\n",
    "                problematic_i.append(i)\n",
    "    return problematic_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 3: Tesla K80 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Graph\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten, Merge, Reshape, Lambda\n",
    "from keras.layers.core import TimeDistributedDense, TimeDistributedMerge\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ParametricSoftplus, ELU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.noise import GaussianDropout, GaussianNoise\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous Ranked Probability Score (used as loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from theano import tensor as T\n",
    "\n",
    "# currently for theano, would need modifications for tensorflow\n",
    "\n",
    "def CRPS(y_true, y_pred):\n",
    "    return K.mean(K.square(T.cumsum(y_pred, axis=-1) - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_sys = Sequential()\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th', input_shape=(30, 196, 196)))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_sys.add(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_sys.add(Dropout(0.2))\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_sys.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_sys.add(Dropout(0.3))\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_sys.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_sys.add(Dropout(0.4))\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_sys.add(BatchNormalization())\n",
    "model_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_sys.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_sys.add(Dropout(0.5))\n",
    "model_sys.add(Flatten())\n",
    "model_sys.add(Dense(2048, activation='relu'))\n",
    "model_sys.add(Dropout(0.5))\n",
    "model_sys.add(Dense(600))\n",
    "model_sys.add(Activation('softmax'))\n",
    "\n",
    "model_sys.compile(loss=CRPS, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5065 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      "106s - loss: 0.0466 - val_loss: 0.0325\n",
      "Epoch 00000: val_loss improved from inf to 0.03247, saving model to ../model_weights/0-convnet_basic_systole.hdf5\n",
      "Epoch 2/100\n",
      "105s - loss: 0.0348 - val_loss: 0.0304\n",
      "Epoch 00001: val_loss improved from 0.03247 to 0.03040, saving model to ../model_weights/0-convnet_basic_systole.hdf5\n",
      "Epoch 3/100\n",
      "106s - loss: 0.0326 - val_loss: 0.0361\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 4/100\n",
      "105s - loss: 0.0306 - val_loss: 0.0322\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 5/100\n",
      "105s - loss: 0.0291 - val_loss: 0.0300\n",
      "Epoch 00004: val_loss improved from 0.03040 to 0.02997, saving model to ../model_weights/0-convnet_basic_systole.hdf5\n",
      "Epoch 6/100\n",
      "105s - loss: 0.0272 - val_loss: 0.0390\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 7/100\n",
      "105s - loss: 0.0266 - val_loss: 0.0324\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 8/100\n",
      "105s - loss: 0.0247 - val_loss: 0.0325\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 9/100\n",
      "105s - loss: 0.0238 - val_loss: 0.0359\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbba562b668>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 100\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath='../model_weights/0-0-convnet_basic_systole.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "model_sys.fit(data_train, label_sys_train, \n",
    "              batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2,\n",
    "              validation_data=(data_train_val, label_sys_train_val), shuffle=True,\n",
    "              callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dia = Sequential()\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th', input_shape=(30, 196, 196)))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_dia.add(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_dia.add(Dropout(0.2))\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_dia.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_dia.add(Dropout(0.3))\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_dia.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_dia.add(Dropout(0.4))\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_dia.add(BatchNormalization())\n",
    "model_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_dia.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_dia.add(Dropout(0.5))\n",
    "model_dia.add(Flatten())\n",
    "model_dia.add(Dense(2048, activation='relu'))\n",
    "model_dia.add(Dropout(0.5))\n",
    "model_dia.add(Dense(600))\n",
    "model_dia.add(Activation('softmax'))\n",
    "\n",
    "model_dia.compile(loss=CRPS, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5065 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      "105s - loss: 0.0590 - val_loss: 0.0452\n",
      "Epoch 00000: val_loss improved from inf to 0.04515, saving model to ../model_weights/0-convnet_basic_diastole.hdf5\n",
      "Epoch 2/100\n",
      "105s - loss: 0.0502 - val_loss: 0.0453\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 3/100\n",
      "105s - loss: 0.0461 - val_loss: 0.0666\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 4/100\n",
      "105s - loss: 0.0434 - val_loss: 0.0463\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 5/100\n",
      "105s - loss: 0.0407 - val_loss: 0.0402\n",
      "Epoch 00004: val_loss improved from 0.04515 to 0.04021, saving model to ../model_weights/0-convnet_basic_diastole.hdf5\n",
      "Epoch 6/100\n",
      "105s - loss: 0.0386 - val_loss: 0.0363\n",
      "Epoch 00005: val_loss improved from 0.04021 to 0.03633, saving model to ../model_weights/0-convnet_basic_diastole.hdf5\n",
      "Epoch 7/100\n",
      "105s - loss: 0.0375 - val_loss: 0.0432\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 8/100\n",
      "105s - loss: 0.0361 - val_loss: 0.0452\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 9/100\n",
      "105s - loss: 0.0341 - val_loss: 0.0414\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 10/100\n",
      "105s - loss: 0.0332 - val_loss: 0.0396\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb38011ef0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 100\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath='../model_weights/0-0-convnet_basic_diastole.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "model_dia.fit(data_train, label_dia_train, \n",
    "              batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2,\n",
    "              validation_data=(data_train_val, label_dia_train_val), shuffle=True,\n",
    "              callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_diff_sys = Sequential()\n",
    "model_diff_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th', input_shape=(29, 196, 196)))\n",
    "model_diff_sys.add(BatchNormalization())\n",
    "model_diff_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_sys.add(BatchNormalization())\n",
    "model_diff_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_sys.add(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_diff_sys.add(Dropout(0.2))\n",
    "model_diff_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_sys.add(BatchNormalization())\n",
    "model_diff_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_sys.add(BatchNormalization())\n",
    "model_diff_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_sys.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_diff_sys.add(Dropout(0.3))\n",
    "model_diff_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_sys.add(BatchNormalization())\n",
    "model_diff_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_sys.add(BatchNormalization())\n",
    "model_diff_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_sys.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_diff_sys.add(Dropout(0.4))\n",
    "model_diff_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_sys.add(BatchNormalization())\n",
    "model_diff_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_sys.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_sys.add(BatchNormalization())\n",
    "model_diff_sys.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_sys.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_diff_sys.add(Dropout(0.5))\n",
    "model_diff_sys.add(Flatten())\n",
    "model_diff_sys.add(Dense(2048, activation='relu'))\n",
    "model_diff_sys.add(Dropout(0.5))\n",
    "model_diff_sys.add(Dense(600))\n",
    "model_diff_sys.add(Activation('softmax'))\n",
    "\n",
    "model_diff_sys.compile(loss=CRPS, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5065 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      "108s - loss: 0.0466 - val_loss: 0.0325\n",
      "Epoch 00000: val_loss improved from inf to 0.03253, saving model to ../model_weights/0-convnet_diffs_systole.hdf5\n",
      "Epoch 2/100\n",
      "105s - loss: 0.0367 - val_loss: 0.0329\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 3/100\n",
      "105s - loss: 0.0360 - val_loss: 0.0346\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 4/100\n",
      "105s - loss: 0.0337 - val_loss: 0.0417\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 5/100\n",
      "105s - loss: 0.0282 - val_loss: 0.0472\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbba562b160>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 100\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath='../model_weights/0-0-convnet_diffs_systole.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "model_diff_sys.fit(data_diffs_train, label_sys_train, \n",
    "                   batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2,\n",
    "                   validation_data=(data_diffs_train_val, label_sys_train_val), shuffle=True,\n",
    "                   callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_diff_dia = Sequential()\n",
    "model_diff_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th', input_shape=(29, 196, 196)))\n",
    "model_diff_dia.add(BatchNormalization())\n",
    "model_diff_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_dia.add(BatchNormalization())\n",
    "model_diff_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_dia.add(MaxPooling2D(pool_size=(2,2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_diff_dia.add(Dropout(0.2))\n",
    "model_diff_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_dia.add(BatchNormalization())\n",
    "model_diff_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_dia.add(BatchNormalization())\n",
    "model_diff_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_dia.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_diff_dia.add(Dropout(0.3))\n",
    "model_diff_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_dia.add(BatchNormalization())\n",
    "model_diff_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_dia.add(BatchNormalization())\n",
    "model_diff_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_dia.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_diff_dia.add(Dropout(0.4))\n",
    "model_diff_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_dia.add(BatchNormalization())\n",
    "model_diff_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_dia.add(Convolution2D(32, 3, 3, init='he_uniform', border_mode='same', dim_ordering='th'))\n",
    "model_diff_dia.add(BatchNormalization())\n",
    "model_diff_dia.add(LeakyReLU(alpha=0.3))\n",
    "model_diff_dia.add(MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th'))\n",
    "model_diff_dia.add(Dropout(0.5))\n",
    "model_diff_dia.add(Flatten())\n",
    "model_diff_dia.add(Dense(2048, activation='relu'))\n",
    "model_diff_dia.add(Dropout(0.5))\n",
    "model_diff_dia.add(Dense(600))\n",
    "model_diff_dia.add(Activation('softmax'))\n",
    "\n",
    "model_diff_dia.compile(loss=CRPS, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5065 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      "105s - loss: 0.0603 - val_loss: 0.0507\n",
      "Epoch 00000: val_loss improved from inf to 0.05065, saving model to ../model_weights/0-convnet_diffs_diastole.hdf5\n",
      "Epoch 2/100\n",
      "105s - loss: 0.0554 - val_loss: 0.0506\n",
      "Epoch 00001: val_loss improved from 0.05065 to 0.05060, saving model to ../model_weights/0-convnet_diffs_diastole.hdf5\n",
      "Epoch 3/100\n",
      "105s - loss: 0.0549 - val_loss: 0.0502\n",
      "Epoch 00002: val_loss improved from 0.05060 to 0.05021, saving model to ../model_weights/0-convnet_diffs_diastole.hdf5\n",
      "Epoch 4/100\n",
      "105s - loss: 0.0540 - val_loss: 0.0507\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 5/100\n",
      "105s - loss: 0.0496 - val_loss: 0.0495\n",
      "Epoch 00004: val_loss improved from 0.05021 to 0.04952, saving model to ../model_weights/0-convnet_diffs_diastole.hdf5\n",
      "Epoch 6/100\n",
      "105s - loss: 0.0418 - val_loss: 0.0572\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 7/100\n",
      "105s - loss: 0.0361 - val_loss: 0.0541\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 8/100\n",
      "105s - loss: 0.0330 - val_loss: 0.0586\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 9/100\n",
      "105s - loss: 0.0302 - val_loss: 0.0569\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb24ae9208>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 100\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath='../model_weights/0-0-convnet_diffs_diastole.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "model_diff_dia.fit(data_diffs_train, label_dia_train, \n",
    "                   batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2,\n",
    "                   validation_data=(data_diffs_train_val, label_dia_train_val), shuffle=True,\n",
    "                   callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_sys.load_weights('../model_weights/0-0-convnet_basic_systole.hdf5')\n",
    "model_dia.load_weights('../model_weights/0-0-convnet_basic_diastole.hdf5')\n",
    "\n",
    "preds_sys = model_sys.predict(data_val, verbose=0)\n",
    "preds_dia = model_dia.predict(data_val, verbose=0)\n",
    "\n",
    "preds_sys = np.clip(np.cumsum(preds_sys, axis=-1), 0, 1)\n",
    "preds_dia = np.clip(np.cumsum(preds_dia, axis=-1), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_sys_pt = {}\n",
    "preds_dia_pt = {}\n",
    "for pt in range(501, 701):\n",
    "    preds_sys_pt[pt] = np.mean(preds_sys[np.where(data_val_pt_index == pt)[0]], axis=0)\n",
    "    preds_dia_pt[pt] = np.mean(preds_dia[np.where(data_val_pt_index == pt)[0]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "with open('../data/sample_submission_validate.csv', 'r') as fi:\n",
    "    reader = csv.reader(fi)\n",
    "    header = next(reader)\n",
    "    \n",
    "    with open('../submissions/0-0-convnet_basic.csv', 'w') as fo:\n",
    "        writer = csv.writer(fo, lineterminator='\\n')\n",
    "        writer.writerow(header)\n",
    "        for rowin in tqdm(reader):\n",
    "            _id = rowin[0]\n",
    "            pt, mode = _id.split('_')\n",
    "            rowout = [_id]\n",
    "            if mode.lower() == 'systole':\n",
    "                rowout.extend(preds_sys_pt[int(pt)].tolist())\n",
    "            elif mode.lower() == 'diastole':\n",
    "                rowout.extend(preds_dia_pt[int(pt)].tolist())\n",
    "            else:\n",
    "                raise\n",
    "            writer.writerow(rowout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
